name: Actualizar noticias PortalPortuario (scraping titulares)

on:
  schedule:
    - cron: '*/5 * * * *'  # intenta cada ~5 minutos (GitHub puede no ejecutar exactamente al minuto)
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configurar Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          python -m pip install beautifulsoup4

      - name: Descargar titulares y generar news.json
        run: |
          python - <<'PY'
          import json
          import re
          import urllib.request
          from datetime import datetime, timezone
          from bs4 import BeautifulSoup

          URL = "https://portalportuario.cl/titulares/"
          OUTPUT = "data/news.json"
          LIMIT = 4

          def fetch_html(url):
            with urllib.request.urlopen(url) as resp:
              return resp.read()

          def clean_text(text):
            if not text:
              return ""
            return " ".join(text.strip().split())

          def extract_summary(node):
            # intenta con párrafos cercanos, si no hay usa título abreviado
            para = node.find("p")
            if para:
              return clean_text(para.get_text())
            return ""

          def extract_image(node):
            # busca primera imagen dentro de la tarjeta
            img = node.find("img")
            if img and img.get("src"):
              return img["src"]
            return None

          html = fetch_html(URL)
          soup = BeautifulSoup(html, "html.parser")

          items = []
          # los titulares suelen estar en <article> o bloques con h2/h3 y enlaces
          for card in soup.find_all(["article", "div"], limit=LIMIT*2):
            title_link = card.find("a")
            if not title_link or not title_link.get("href") or not title_link.get_text(strip=True):
              continue

            title = clean_text(title_link.get_text())
            link = title_link["href"]

            # fecha: puede estar en un <time> o texto con día
            pub_date = ""
            time_tag = card.find("time")
            if time_tag and time_tag.get("datetime"):
              pub_date = time_tag["datetime"]
            elif time_tag and time_tag.get_text(strip=True):
              pub_date = time_tag.get_text(strip=True)

            summary = extract_summary(card)
            image = extract_image(card)

            items.append({
              "title": title,
              "link": link,
              "pubDate": pub_date,
              "summary": summary or title[:180] + ("…" if len(title) > 180 else ""),
              "image": image
            })

            if len(items) >= LIMIT:
              break

          # fallback si no hay fecha: se coloca hora actual en UTC
          now_iso = datetime.now(timezone.utc).isoformat()
          for it in items:
            if not it.get("pubDate"):
              it["pubDate"] = now_iso

          payload = {
            "lastUpdated": now_iso,
            "items": items
          }

          with open(OUTPUT, "w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
          PY

      - name: Commit y push si hay cambios
        run: |
          git config user.name "github-actions"
          git config user.email "actions@users.noreply.github.com"
          if ! git diff --quiet; then
            git add data/news.json
            git commit -m "chore: actualizar noticias por scraping de titulares"
            git push
          else
            echo "Sin cambios que publicar"
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
